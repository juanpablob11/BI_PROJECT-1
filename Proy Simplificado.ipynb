{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncwsXz1pOXLB"
      },
      "source": [
        "# **PROYECTO 1 BI - DETECCIÓN DE FAKE NEWS**\n",
        "---\n",
        "\n",
        "Integrantes equipo #11:\n",
        "*   Estudiante #1: Juan Pablo Barón - 202210502\n",
        "*   Estudiante #2: María José Amorocho - 202220179\n",
        "*   Estudiante #3: Julian Mondragón - 202221122\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 0. Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Label</th>\n",
              "      <th>Titulo</th>\n",
              "      <th>Descripcion</th>\n",
              "      <th>Fecha</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ID</td>\n",
              "      <td>1</td>\n",
              "      <td>'The Guardian' va con Sánchez: 'Europa necesit...</td>\n",
              "      <td>El diario británico publicó este pasado jueves...</td>\n",
              "      <td>02/06/2023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ID</td>\n",
              "      <td>0</td>\n",
              "      <td>REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...</td>\n",
              "      <td>REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...</td>\n",
              "      <td>01/10/2023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ID</td>\n",
              "      <td>1</td>\n",
              "      <td>El 'Ahora o nunca' de Joan Fuster sobre el est...</td>\n",
              "      <td>El valencianismo convoca en Castelló su fiesta...</td>\n",
              "      <td>25/04/2022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ID</td>\n",
              "      <td>1</td>\n",
              "      <td>Iglesias alienta a Yolanda Díaz, ERC y EH Bild...</td>\n",
              "      <td>En política, igual que hay que negociar con lo...</td>\n",
              "      <td>03/01/2022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ID</td>\n",
              "      <td>0</td>\n",
              "      <td>Puigdemont: 'No sería ninguna tragedia una rep...</td>\n",
              "      <td>En una entrevista en El Punt Avui, el líder de...</td>\n",
              "      <td>09/03/2018</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID  Label                                             Titulo  \\\n",
              "0  ID      1  'The Guardian' va con Sánchez: 'Europa necesit...   \n",
              "1  ID      0  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...   \n",
              "2  ID      1  El 'Ahora o nunca' de Joan Fuster sobre el est...   \n",
              "3  ID      1  Iglesias alienta a Yolanda Díaz, ERC y EH Bild...   \n",
              "4  ID      0  Puigdemont: 'No sería ninguna tragedia una rep...   \n",
              "\n",
              "                                         Descripcion       Fecha  \n",
              "0  El diario británico publicó este pasado jueves...  02/06/2023  \n",
              "1  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...  01/10/2023  \n",
              "2  El valencianismo convoca en Castelló su fiesta...  25/04/2022  \n",
              "3  En política, igual que hay que negociar con lo...  03/01/2022  \n",
              "4  En una entrevista en El Punt Avui, el líder de...  09/03/2018  "
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pandas import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import re, string, unicodedata\n",
        "from num2words import num2words\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "\n",
        "\n",
        "# Punkt permite separar un texto en frases.\n",
        "#nltk.download('punkt')\n",
        "#Descargar palabras vacías (stopwords)\n",
        "#nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"spanish\"))\n",
        "\n",
        "\n",
        "data_set = pd.read_csv('./Data/fake_news_spanish.csv', delimiter=\";\")\n",
        "data_set_augmented = pd.read_csv('./Data/Noticias_Falsas_Extendidas_50-50.csv', delimiter=\",\")\n",
        "data_set_augmented_copy = data_set_augmented.copy()\n",
        "\n",
        "data_set_augmented.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs-0HUA1P8NJ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " # **2. Entendimiento y preparación de los datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Selección de variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "def definir_variables(data_set_inicial):\n",
        "    features = ['Titulo', 'Descripcion', 'Label']\n",
        "    return data_set_inicial[features]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HywvPf5FQi0x"
      },
      "source": [
        "### 2.3 Preparación de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.3.1 Limpieza de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_duplicates(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Elimina las filas duplicadas del dataSet.\n",
        "    \n",
        "    Parámetros:\n",
        "    df (pd.DataFrame): DataFrame de entrada.\n",
        "    \n",
        "    Retorna:\n",
        "    pd.DataFrame: DataFrame sin filas duplicadas.\n",
        "    \"\"\"\n",
        "    return df.drop_duplicates().reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eliminar_duplicados_parciales(df):\n",
        "    \"\"\"\n",
        "    Elimina filas donde 'Titulo' y 'Descripcion' sean iguales, pero 'Label' sea diferente.\n",
        "    \n",
        "    Parámetros:\n",
        "    df (pd.DataFrame): DataFrame con las columnas 'Titulo', 'Descripcion' y 'Label'.\n",
        "    \n",
        "    Retorna:\n",
        "    pd.DataFrame: DataFrame sin los duplicados parciales.\n",
        "    \"\"\"\n",
        "    # Contar cuántos valores únicos de Label existen por cada combinación de Titulo y Descripcion\n",
        "    conteo_labels = df.groupby(['Titulo', 'Descripcion'])['Label'].nunique()\n",
        "    \n",
        "    # Identificar las combinaciones que tienen más de un Label distinto (es decir, duplicados parciales)\n",
        "    duplicados_parciales = conteo_labels[conteo_labels > 1].index\n",
        "    \n",
        "    # Filtrar el DataFrame eliminando estas combinaciones\n",
        "    df_filtrado = df[~df.set_index(['Titulo', 'Descripcion']).index.isin(duplicados_parciales)]\n",
        "    \n",
        "    return df_filtrado.reset_index(drop=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "def limpiar_data(data_set):\n",
        "    df_variables = definir_variables(data_set)\n",
        "    df_duplicados = remove_duplicates(df_variables)\n",
        "    df_limpio = eliminar_duplicados_parciales(df_duplicados)\n",
        "    return df_limpio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.3.3 Tokenización, lematización y normalización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "import swifter\n",
        "\n",
        "# Cargar modelo spaCy sin parser ni NER para mayor velocidad\n",
        "nlp = spacy.load(\"es_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
        "stop_words = set(stopwords.words(\"spanish\"))\n",
        "\n",
        "def preprocessing2(words):\n",
        "    words = [word.lower() for word in words]  # Convertir a minúsculas directamente\n",
        "    words = [num2words(word, lang=\"es\") if word.isdigit() else word for word in words]  # Reemplazar números\n",
        "    words = [re.sub(r'[^\\w\\s]', '', word) for word in words if word]  # Eliminar puntuación\n",
        "    words = [unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore') for word in words]  # Quitar caracteres no ASCII\n",
        "    words = [word for word in words if word not in stop_words]  # Remover stopwords\n",
        "    words = [word for word in words if word.strip() != \"\"] \n",
        "    return words\n",
        "\n",
        "def procesar_texto(df, columnas):\n",
        "    \"\"\"\n",
        "    Optimiza la tokenización, lematización y preprocesamiento de múltiples columnas de un DataFrame.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df[columnas] = df[columnas].astype(str).fillna(\"\")\n",
        "    \n",
        "    datos_procesados = {}\n",
        "    \n",
        "    for col in columnas:\n",
        "        resultados = [\n",
        "            ([token.text for token in doc if not token.is_space],  # Tokens\n",
        "             [token.lemma_ for token in doc if not token.is_space])  # Lemas\n",
        "            for doc in nlp.pipe(df[col], batch_size=100)  # Mayor batch_size para eficiencia\n",
        "        ]\n",
        "        \n",
        "        # Separar tokens y lemas en listas\n",
        "        df[f\"{col}_tokens\"], df[f\"{col}_lemmas\"] = zip(*resultados)\n",
        "        \n",
        "        # Aplicar procesamiento en paralelo\n",
        "        datos_procesados[f\"{col}_tokens_clean\"] = df[f\"{col}_tokens\"].swifter.apply(preprocessing2)\n",
        "    \n",
        "    # Crear nuevo DataFrame solo con las columnas procesadas + label\n",
        "    nuevo_df = pd.DataFrame(datos_procesados)\n",
        "    nuevo_df.insert(0, \"Label\", df[\"Label\"])  # Mantener la etiqueta\n",
        "    \n",
        "    return nuevo_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Data limpia\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Pandas Apply: 100%|██████████| 64062/64062 [00:03<00:00, 17552.73it/s]\n",
            "Pandas Apply: 100%|██████████| 64062/64062 [00:26<00:00, 2440.29it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Titulo_tokens_clean</th>\n",
              "      <th>Descripcion_tokens_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>[the, guardian, va, sanchez, europa, necesita,...</td>\n",
              "      <td>[diario, britanico, publico, pasado, jueves, e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>[revelan, gobierno, negocio, liberacion, mirel...</td>\n",
              "      <td>[revelan, gobierno, negocio, liberacion, mirel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>[ahora, nunca, joan, fuster, estatuto, valenci...</td>\n",
              "      <td>[valencianismo, convoca, castello, fiesta, gra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>[iglesias, alienta, yolanda, diaz, erc, eh, bi...</td>\n",
              "      <td>[politica, igual, negociar, empresarios, negoc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>[puigdemont, seria, ninguna, tragedia, repetic...</td>\n",
              "      <td>[entrevista, punt, avui, lider, jxcat, desdram...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>[pnv, consolida, mayoria, pse, salva, papeles,...</td>\n",
              "      <td>[nacionalistas, consiguen, alcaldias, bilbao, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>[exconsejero, nuria, marin, pide, indulto, cas...</td>\n",
              "      <td>[familiares, aluden, honestidad, integridad, p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>[fiscalia, pide, prision, incondicional, siete...</td>\n",
              "      <td>[suprime, delito, rebelion, imputo, inicialmen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>[jose, manuel, perez, tornero, creador, televi...</td>\n",
              "      <td>[futuro, presidente, rtve, licenciado, ciencia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>[ayusizacion, bng, santiago, abascal, instruye...</td>\n",
              "      <td>[pablo, santiago, abascal, planea, vivir, rent...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Label                                Titulo_tokens_clean  \\\n",
              "0      1  [the, guardian, va, sanchez, europa, necesita,...   \n",
              "1      0  [revelan, gobierno, negocio, liberacion, mirel...   \n",
              "2      1  [ahora, nunca, joan, fuster, estatuto, valenci...   \n",
              "3      1  [iglesias, alienta, yolanda, diaz, erc, eh, bi...   \n",
              "4      0  [puigdemont, seria, ninguna, tragedia, repetic...   \n",
              "5      1  [pnv, consolida, mayoria, pse, salva, papeles,...   \n",
              "6      0  [exconsejero, nuria, marin, pide, indulto, cas...   \n",
              "7      1  [fiscalia, pide, prision, incondicional, siete...   \n",
              "8      1  [jose, manuel, perez, tornero, creador, televi...   \n",
              "9      0  [ayusizacion, bng, santiago, abascal, instruye...   \n",
              "\n",
              "                            Descripcion_tokens_clean  \n",
              "0  [diario, britanico, publico, pasado, jueves, e...  \n",
              "1  [revelan, gobierno, negocio, liberacion, mirel...  \n",
              "2  [valencianismo, convoca, castello, fiesta, gra...  \n",
              "3  [politica, igual, negociar, empresarios, negoc...  \n",
              "4  [entrevista, punt, avui, lider, jxcat, desdram...  \n",
              "5  [nacionalistas, consiguen, alcaldias, bilbao, ...  \n",
              "6  [familiares, aluden, honestidad, integridad, p...  \n",
              "7  [suprime, delito, rebelion, imputo, inicialmen...  \n",
              "8  [futuro, presidente, rtve, licenciado, ciencia...  \n",
              "9  [pablo, santiago, abascal, planea, vivir, rent...  "
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def limpiar_y_procesar(data_set, columnas_texto):\n",
        "    \"\"\"\n",
        "    Función que aplica limpieza de datos y tokenización en columnas de texto específicas.\n",
        "\n",
        "    Parámetros:\n",
        "    - data_set (DataFrame): Dataset original con las columnas a procesar.\n",
        "    - columnas_texto (list): Lista de nombres de columnas que contienen texto a limpiar.\n",
        "\n",
        "    Retorna:\n",
        "    - DataFrame con los textos procesados.\n",
        "    \"\"\"\n",
        "    # Aplicar limpieza de datos\n",
        "    data_set_limpio = limpiar_data(data_set)\n",
        "    print(\" Data limpia\")\n",
        "\n",
        "    # Aplicar tokenización, lematización y normalización\n",
        "    df_procesado = procesar_texto(data_set_limpio, columnas_texto)\n",
        "    \n",
        "    return df_procesado\n",
        "\n",
        "# Uso de la función\n",
        "# df_procesado = limpiar_y_procesar(data_set, [\"Titulo\", \"Descripcion\"])\n",
        "df_procesado = limpiar_y_procesar(data_set_augmented, [\"Titulo\", \"Descripcion\"])\n",
        "df_procesado.head(10)\n",
        "\n",
        "# df = limpiar_y_procesar(data_set, [\"Titulo\", \"Descripcion\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.3.4 Vectorización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                 Titulo_tokens_clean  \\\n",
            "0  the guardian va sanchez europa necesita apuest...   \n",
            "1  revelan gobierno negocio liberacion mireles ca...   \n",
            "2  ahora nunca joan fuster estatuto valenciano cu...   \n",
            "3  iglesias alienta yolanda diaz erc eh bildu neg...   \n",
            "4  puigdemont seria ninguna tragedia repeticion e...   \n",
            "\n",
            "                            Descripcion_tokens_clean  \n",
            "0  diario britanico publico pasado jueves editori...  \n",
            "1  revelan gobierno negocio liberacion mireles ca...  \n",
            "2  valencianismo convoca castello fiesta grande c...  \n",
            "3  politica igual negociar empresarios negociar g...  \n",
            "4  entrevista punt avui lider jxcat desdramatizad...  \n"
          ]
        }
      ],
      "source": [
        "# Convertir listas de tokens a strings correctamente\n",
        "df_procesado[\"Titulo_tokens_clean\"] = df_procesado[\"Titulo_tokens_clean\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)\n",
        "df_procesado[\"Descripcion_tokens_clean\"] = df_procesado[\"Descripcion_tokens_clean\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)\n",
        "\n",
        "print(df_procesado[[\"Titulo_tokens_clean\", \"Descripcion_tokens_clean\"]].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Clase pipeline limpieza y preprocesamiento**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class LimpiezaPreprocesamiento1(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        from nltk.corpus import stopwords\n",
        "        self.stop_words = set(stopwords.words(\"spanish\"))\n",
        "\n",
        "    def definir_variables(self, df):\n",
        "        features = ['Titulo', 'Descripcion', 'Label']\n",
        "        return df[features] if \"Label\" in df.columns else df[[\"Titulo\", \"Descripcion\"]]\n",
        "\n",
        "    def remove_duplicates(self, df):\n",
        "        return df.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "    def eliminar_duplicados_parciales(self, df):\n",
        "        if \"Label\" not in df.columns:\n",
        "            return df\n",
        "        conteo_labels = df.groupby(['Titulo', 'Descripcion'])['Label'].nunique()\n",
        "        duplicados_parciales = conteo_labels[conteo_labels > 1].index\n",
        "        return df[~df.set_index(['Titulo', 'Descripcion']).index.isin(duplicados_parciales)].reset_index(drop=True)\n",
        "\n",
        "    def limpiar_data(self, df):\n",
        "        df_variables = self.definir_variables(df)\n",
        "        df_duplicados = self.remove_duplicates(df_variables)\n",
        "        return self.eliminar_duplicados_parciales(df_duplicados)\n",
        "\n",
        "    def preprocessing(self, words):\n",
        "        import re, unicodedata\n",
        "        from num2words import num2words\n",
        "\n",
        "        words = [word.lower() for word in words]\n",
        "        words = [num2words(word, lang=\"es\") if word.isdigit() else word for word in words]\n",
        "        words = [re.sub(r'[^\\w\\s]', '', word) for word in words if word]\n",
        "        words = [unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore') for word in words]\n",
        "        words = [word for word in words if word not in self.stop_words]\n",
        "        return [word for word in words if word.strip() != \"\"]\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        import pandas as pd\n",
        "        import spacy\n",
        "\n",
        "        df = X.copy()\n",
        "        df = self.limpiar_data(df)\n",
        "\n",
        "        df[\"Titulo\"] = df[\"Titulo\"].astype(str).fillna(\"\")\n",
        "        df[\"Descripcion\"] = df[\"Descripcion\"].astype(str).fillna(\"\")\n",
        "\n",
        "        nlp = spacy.load(\"es_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
        "\n",
        "        resultados_titulo = [\n",
        "            [token.text for token in doc if not token.is_space]\n",
        "            for doc in nlp.pipe(df[\"Titulo\"], batch_size=100)\n",
        "        ]\n",
        "        resultados_descripcion = [\n",
        "            [token.text for token in doc if not token.is_space]\n",
        "            for doc in nlp.pipe(df[\"Descripcion\"], batch_size=100)\n",
        "        ]\n",
        "\n",
        "        df[\"Titulo_tokens_clean\"] = [self.preprocessing(t) for t in resultados_titulo]\n",
        "        df[\"Descripcion_tokens_clean\"] = [self.preprocessing(d) for d in resultados_descripcion]\n",
        "\n",
        "        df[\"Titulo_tokens_clean\"] = df[\"Titulo_tokens_clean\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)\n",
        "        df[\"Descripcion_tokens_clean\"] = df[\"Descripcion_tokens_clean\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)\n",
        "\n",
        "        return df[[\"Titulo_tokens_clean\", \"Descripcion_tokens_clean\"]]\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Dataset de entrenamiento: 47062 registros\n",
            "✅ Dataset de prueba uniforme (17k registros): 17000 registros (8500 fake news y 8500 verídicas)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\majoa\\AppData\\Local\\Temp\\ipykernel_7416\\5412614.py:7: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.sample(n=n_samples, random_state=42)))\n"
          ]
        }
      ],
      "source": [
        "# Definir la cantidad de muestras por clase\n",
        "#df_procesado = data_set_augmented_copy\n",
        "n_samples = 8500 #LO MOVI\n",
        "\n",
        "# Extraer 8500 noticias falsas y 8500 noticias verídicas para pruebas finales\n",
        "df_test_uniform = (df_procesado.groupby(\"Label\", group_keys=False)\n",
        "                    .apply(lambda x: x.sample(n=n_samples, random_state=42)))\n",
        "\n",
        "# Crear el nuevo dataset de entrenamiento sin los registros seleccionados para pruebas\n",
        "df_train = df_procesado.drop(df_test_uniform.index)\n",
        "\n",
        "# Verificar tamaños de los datasets\n",
        "print(f\"✅ Dataset de entrenamiento: {len(df_train)} registros\")\n",
        "print(f\"✅ Dataset de prueba uniforme (17k registros): {len(df_test_uniform)} registros (8500 fake news y 8500 verídicas)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para la vectorización de textos se hace uso de la librería TF-IDF (Term Frequency-Inverse Document Frequency). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de registros en X: 64062\n",
            "Número de etiquetas en y: 64062\n",
            " X y y tienen el mismo número de registros. OK.\n"
          ]
        }
      ],
      "source": [
        "from scipy.sparse import hstack\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Vectorización con TF-IDF\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=3000)\n",
        "X_titulo = vectorizer.fit_transform(df_procesado[\"Titulo_tokens_clean\"])\n",
        "X_descripcion = vectorizer.fit_transform(df_procesado[\"Descripcion_tokens_clean\"])\n",
        "\n",
        "#  Concatenar ambas representaciones en una sola matriz\n",
        "X = hstack([X_titulo, X_descripcion])\n",
        "\n",
        "# Etiquetas\n",
        "y = df_procesado[\"Label\"]\n",
        "\n",
        "# Verificar dimensiones de la matriz X y etiquetas y\n",
        "print(f\"Número de registros en X: {X.shape[0]}\")\n",
        "print(f\"Número de etiquetas en y: {y.shape[0]}\")\n",
        "\n",
        "# Comprobar si hay inconsistencia\n",
        "if X.shape[0] == y.shape[0]:\n",
        "    print(\" X y y tienen el mismo número de registros. OK.\")\n",
        "else:\n",
        "    print(\" Error: X y y tienen un número diferente de registros.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usw9mrH3Qp8Y"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **3. Modelado y evaluación**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jal7SSSFRCvo"
      },
      "source": [
        "### 3.3. Estudiante 3 (Julian Mondragón): Modelo 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "B1PZZ_hxRIrn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Ejecutando Cross-Validation con Random Forest usando df_train...\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "\"['Titulo', 'Descripcion'] not in index\"",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[105], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Cross-validation donde `TfidfVectorizer` se ajusta dentro de cada fold\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Ejecutando Cross-Validation con Random Forest usando df_train...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m y_pred_cv \u001b[38;5;241m=\u001b[39m cross_val_predict(modelo_rf, df_train, df_train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m], cv\u001b[38;5;241m=\u001b[39mkf, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Calcular métricas en cross-validation\u001b[39;00m\n\u001b[0;32m     38\u001b[0m accuracy_cv \u001b[38;5;241m=\u001b[39m accuracy_score(df_train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m], y_pred_cv)\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1247\u001b[0m, in \u001b[0;36mcross_val_predict\u001b[1;34m(estimator, X, y, groups, cv, n_jobs, verbose, params, pre_dispatch, method)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m-> 1247\u001b[0m predictions \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m   1248\u001b[0m     delayed(_fit_and_predict)(\n\u001b[0;32m   1249\u001b[0m         clone(estimator),\n\u001b[0;32m   1250\u001b[0m         X,\n\u001b[0;32m   1251\u001b[0m         y,\n\u001b[0;32m   1252\u001b[0m         train,\n\u001b[0;32m   1253\u001b[0m         test,\n\u001b[0;32m   1254\u001b[0m         routed_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit,\n\u001b[0;32m   1255\u001b[0m         method,\n\u001b[0;32m   1256\u001b[0m     )\n\u001b[0;32m   1257\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m splits\n\u001b[0;32m   1258\u001b[0m )\n\u001b[0;32m   1260\u001b[0m inv_test_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(test_indices), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m   1261\u001b[0m inv_test_indices[test_indices] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(test_indices))\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1332\u001b[0m, in \u001b[0;36m_fit_and_predict\u001b[1;34m(estimator, X, y, train, test, fit_params, method)\u001b[0m\n\u001b[0;32m   1330\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1332\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m   1333\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(estimator, method)\n\u001b[0;32m   1334\u001b[0m predictions \u001b[38;5;241m=\u001b[39m func(X_test)\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\sklearn\\pipeline.py:654\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    648\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    649\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    650\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    651\u001b[0m     )\n\u001b[0;32m    653\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 654\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, routed_params, raw_params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\sklearn\\pipeline.py:588\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m    582\u001b[0m step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    583\u001b[0m     step_idx\u001b[38;5;241m=\u001b[39mstep_idx,\n\u001b[0;32m    584\u001b[0m     step_params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[0;32m    585\u001b[0m     all_params\u001b[38;5;241m=\u001b[39mraw_params,\n\u001b[0;32m    586\u001b[0m )\n\u001b[1;32m--> 588\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    589\u001b[0m     cloned_transformer,\n\u001b[0;32m    590\u001b[0m     X,\n\u001b[0;32m    591\u001b[0m     y,\n\u001b[0;32m    592\u001b[0m     weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    593\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    594\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    595\u001b[0m     params\u001b[38;5;241m=\u001b[39mstep_params,\n\u001b[0;32m    596\u001b[0m )\n\u001b[0;32m    597\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\joblib\\memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\sklearn\\pipeline.py:1551\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1551\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1553\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1554\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1555\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\sklearn\\base.py:921\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\majoa\\Documents\\Universidad\\SextoSemestre\\BI\\BI_PROJECT-1\\limpieza_transformer.py:44\u001b[0m, in \u001b[0;36mLimpiezaPreprocesamiento.transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m     43\u001b[0m df \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 44\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlimpiar_data(df)\n\u001b[0;32m     46\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitulo\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitulo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescripcion\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescripcion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\majoa\\Documents\\Universidad\\SextoSemestre\\BI\\BI_PROJECT-1\\limpieza_transformer.py:24\u001b[0m, in \u001b[0;36mLimpiezaPreprocesamiento.limpiar_data\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlimpiar_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, df):\n\u001b[1;32m---> 24\u001b[0m     df_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefinir_variables(df)\n\u001b[0;32m     25\u001b[0m     df_duplicados \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremove_duplicates(df_variables)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meliminar_duplicados_parciales(df_duplicados)\n",
            "File \u001b[1;32mc:\\Users\\majoa\\Documents\\Universidad\\SextoSemestre\\BI\\BI_PROJECT-1\\limpieza_transformer.py:11\u001b[0m, in \u001b[0;36mLimpiezaPreprocesamiento.definir_variables\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefinir_variables\u001b[39m(\u001b[38;5;28mself\u001b[39m, df):\n\u001b[0;32m     10\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitulo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescripcion\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[features] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01melse\u001b[39;00m df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitulo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescripcion\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mKeyError\u001b[0m: \"['Titulo', 'Descripcion'] not in index\""
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import time\n",
        "from limpieza_transformer import LimpiezaPreprocesamiento\n",
        "import joblib\n",
        "\n",
        "# Medir tiempo de ejecución\n",
        "start_time = time.time()\n",
        "\n",
        "# SEPARAR el preprocesamiento de la vectorización \n",
        "tfidf_transformer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('tfidf_titulo', TfidfVectorizer(ngram_range=(1,3), max_features=6000), \"Titulo_tokens_clean\"),\n",
        "        ('tfidf_descripcion', TfidfVectorizer(ngram_range=(1,3), max_features=6000), \"Descripcion_tokens_clean\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Crear pipeline donde el vectorizador **solo se ajusta dentro de cada fold**\n",
        "modelo_rf = Pipeline([\n",
        "    ('limpieza', LimpiezaPreprocesamiento()),\n",
        "    ('vectorizacion', tfidf_transformer),\n",
        "    ('clasificador', RandomForestClassifier(n_estimators=100, random_state=42))  \n",
        "])\n",
        "\n",
        "# Definir K-Folds estratificado (10 folds)\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Cross-validation donde `TfidfVectorizer` se ajusta dentro de cada fold\n",
        "print(\" Ejecutando Cross-Validation con Random Forest usando df_train...\")\n",
        "\n",
        "y_pred_cv = cross_val_predict(modelo_rf, df_train, df_train[\"Label\"], cv=kf, n_jobs=1, verbose=1)\n",
        "\n",
        "# Calcular métricas en cross-validation\n",
        "accuracy_cv = accuracy_score(df_train[\"Label\"], y_pred_cv)\n",
        "precision_cv = precision_score(df_train[\"Label\"], y_pred_cv, average='binary')\n",
        "recall_cv = recall_score(df_train[\"Label\"], y_pred_cv, average='binary')\n",
        "f1_cv = f1_score(df_train[\"Label\"], y_pred_cv, average='binary')\n",
        "conf_matrix_cv = confusion_matrix(df_train[\"Label\"], y_pred_cv)\n",
        "\n",
        "# Mostrar resultados de validación cruzada\n",
        "print(\"\\n Resultados de Cross-Validation con Random Forest en df_train:\")\n",
        "print(f\" Accuracy: {accuracy_cv:.4f}\")\n",
        "print(f\" Precision: {precision_cv:.4f}\")\n",
        "print(f\" Recall: {recall_cv:.4f}\")\n",
        "print(f\" F1-Score: {f1_cv:.4f}\")\n",
        "print(\"\\n Matriz de Confusión:\")\n",
        "print(conf_matrix_cv)\n",
        "\n",
        "# Reporte de clasificación\n",
        "print(\"\\n Reporte de Clasificación en Cross-Validation:\")\n",
        "print(classification_report(df_train[\"Label\"], y_pred_cv))\n",
        "\n",
        "# Medir tiempo total de entrenamiento\n",
        "end_time = time.time()\n",
        "print(f\"\\n Tiempo total de entrenamiento: {(end_time - start_time) / 60:.2f} minutos\")\n",
        "\n",
        "# -------------------------------------------\n",
        "# **ENTRENAMIENTO FINAL con df_train y evaluación en df_test_uniform**\n",
        "# -------------------------------------------\n",
        "\n",
        "# Ahora entrenamos el modelo en todo df_train (sin test leakage)\n",
        "modelo_rf.fit(df_train, df_train[\"Label\"])\n",
        "# Guardar el pipeline en un archivo\n",
        "joblib.dump(modelo_rf, \"modelo_random_forest.pkl\")\n",
        "\n",
        "# Transformamos df_test_uniform usando los valores aprendidos en df_train\n",
        "y_pred_test = modelo_rf.predict(df_test_uniform)\n",
        "\n",
        "# Calcular métricas en prueba final\n",
        "accuracy_test = accuracy_score(df_test_uniform[\"Label\"], y_pred_test)\n",
        "precision_test = precision_score(df_test_uniform[\"Label\"], y_pred_test, average='binary')\n",
        "recall_test = recall_score(df_test_uniform[\"Label\"], y_pred_test, average='binary')\n",
        "f1_test = f1_score(df_test_uniform[\"Label\"], y_pred_test, average='binary')\n",
        "conf_matrix_test = confusion_matrix(df_test_uniform[\"Label\"], y_pred_test)\n",
        "\n",
        "# Mostrar resultados en el conjunto de prueba final\n",
        "print(\"\\nResultados en el **Conjunto de Prueba Final (df_test_uniform)**:\")\n",
        "print(f\" Accuracy: {accuracy_test:.4f}\")\n",
        "print(f\" Precision: {precision_test:.4f}\")\n",
        "print(f\" Recall: {recall_test:.4f}\")\n",
        "print(f\" F1-Score: {f1_test:.4f}\")\n",
        "print(\"\\n Matriz de Confusión:\")\n",
        "print(conf_matrix_test)\n",
        "\n",
        "# Reporte de clasificación en prueba final\n",
        "print(\"\\n Reporte de Clasificación en df_test_uniform:\")\n",
        "print(classification_report(df_test_uniform[\"Label\"], y_pred_test))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "uniandes",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
