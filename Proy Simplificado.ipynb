{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncwsXz1pOXLB"
      },
      "source": [
        "# **PROYECTO 1 BI - DETECCIÓN DE FAKE NEWS**\n",
        "---\n",
        "\n",
        "Integrantes equipo #11:\n",
        "*   Estudiante #1: Juan Pablo Barón - 202210502\n",
        "*   Estudiante #2: María José Amorocho - 202220179\n",
        "*   Estudiante #3: Julian Mondragón - 202221122\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 0. Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1988, 5)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pandas import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import re, string, unicodedata\n",
        "from num2words import num2words\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "\n",
        "\n",
        "# Punkt permite separar un texto en frases.\n",
        "#nltk.download('punkt')\n",
        "#Descargar palabras vacías (stopwords)\n",
        "#nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"spanish\"))\n",
        "\n",
        "\n",
        "data_set = pd.read_csv('./Data/fake_news_spanish-2k.csv', delimiter=\";\")\n",
        "data_set_augmented = pd.read_csv('./Data/Noticias_Falsas_Extendidas_50-50.csv', delimiter=\",\")\n",
        "data_set_augmented.head()\n",
        "\n",
        "data_set_augmented = data_set\n",
        "data_set_augmented.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs-0HUA1P8NJ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " # **2. Entendimiento y preparación de los datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Selección de variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def definir_variables(data_set_inicial):\n",
        "    features = ['Titulo', 'Descripcion', 'Label']\n",
        "    return data_set_inicial[features]\n",
        "    \n",
        "#df = definir_variables(data_set)\n",
        "#df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HywvPf5FQi0x"
      },
      "source": [
        "### 2.3 Preparación de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.3.1 Limpieza de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_duplicates(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Elimina las filas duplicadas del dataSet.\n",
        "    \n",
        "    Parámetros:\n",
        "    df (pd.DataFrame): DataFrame de entrada.\n",
        "    \n",
        "    Retorna:\n",
        "    pd.DataFrame: DataFrame sin filas duplicadas.\n",
        "    \"\"\"\n",
        "    return df.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "#df = remove_duplicates(data_set)\n",
        "#df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eliminar_duplicados_parciales(df):\n",
        "    \"\"\"\n",
        "    Elimina filas donde 'Titulo' y 'Descripcion' sean iguales, pero 'Label' sea diferente.\n",
        "    \n",
        "    Parámetros:\n",
        "    df (pd.DataFrame): DataFrame con las columnas 'Titulo', 'Descripcion' y 'Label'.\n",
        "    \n",
        "    Retorna:\n",
        "    pd.DataFrame: DataFrame sin los duplicados parciales.\n",
        "    \"\"\"\n",
        "    # Contar cuántos valores únicos de Label existen por cada combinación de Titulo y Descripcion\n",
        "    conteo_labels = df.groupby(['Titulo', 'Descripcion'])['Label'].nunique()\n",
        "    \n",
        "    # Identificar las combinaciones que tienen más de un Label distinto (es decir, duplicados parciales)\n",
        "    duplicados_parciales = conteo_labels[conteo_labels > 1].index\n",
        "    \n",
        "    # Filtrar el DataFrame eliminando estas combinaciones\n",
        "    df_filtrado = df[~df.set_index(['Titulo', 'Descripcion']).index.isin(duplicados_parciales)]\n",
        "    \n",
        "    return df_filtrado.reset_index(drop=True)\n",
        "\n",
        "#df = eliminar_duplicados_parciales(data_set)\n",
        "#df.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A continuación, se muestra un método para hacer la limpieza completa de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1988, 3)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def limpiar_data(data_set):\n",
        "    df_variables = definir_variables(data_set)\n",
        "    df_duplicados = remove_duplicates(df_variables)\n",
        "    df_limpio = eliminar_duplicados_parciales(df_duplicados)\n",
        "    return df_limpio\n",
        "\n",
        "df = limpiar_data(data_set)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.3.3 Tokenización, lematización y normalización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "import swifter\n",
        "\n",
        "# Cargar modelo spaCy sin parser ni NER para mayor velocidad\n",
        "nlp = spacy.load(\"es_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
        "stop_words = set(stopwords.words(\"spanish\"))\n",
        "\n",
        "def preprocessing2(words):\n",
        "    words = [word.lower() for word in words]  # Convertir a minúsculas directamente\n",
        "    words = [num2words(word, lang=\"es\") if word.isdigit() else word for word in words]  # Reemplazar números\n",
        "    words = [re.sub(r'[^\\w\\s]', '', word) for word in words if word]  # Eliminar puntuación\n",
        "    words = [unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore') for word in words]  # Quitar caracteres no ASCII\n",
        "    words = [word for word in words if word not in stop_words]  # Remover stopwords\n",
        "    words = [word for word in words if word.strip() != \"\"] \n",
        "    return words\n",
        "\n",
        "def procesar_texto(df, columnas):\n",
        "    \"\"\"\n",
        "    Optimiza la tokenización, lematización y preprocesamiento de múltiples columnas de un DataFrame.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df[columnas] = df[columnas].astype(str).fillna(\"\")\n",
        "    \n",
        "    datos_procesados = {}\n",
        "    \n",
        "    for col in columnas:\n",
        "        resultados = [\n",
        "            ([token.text for token in doc if not token.is_space],  # Tokens\n",
        "             [token.lemma_ for token in doc if not token.is_space])  # Lemas\n",
        "            for doc in nlp.pipe(df[col], batch_size=100)  # Mayor batch_size para eficiencia\n",
        "        ]\n",
        "        \n",
        "        # Separar tokens y lemas en listas\n",
        "        df[f\"{col}_tokens\"], df[f\"{col}_lemmas\"] = zip(*resultados)\n",
        "        \n",
        "        # Aplicar procesamiento en paralelo\n",
        "        datos_procesados[f\"{col}_tokens_clean\"] = df[f\"{col}_tokens\"].swifter.apply(preprocessing2)\n",
        "    \n",
        "    # Crear nuevo DataFrame solo con las columnas procesadas + label\n",
        "    nuevo_df = pd.DataFrame(datos_procesados)\n",
        "    nuevo_df.insert(0, \"Label\", df[\"Label\"])  # Mantener la etiqueta\n",
        "    \n",
        "    return nuevo_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Data limpia\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Pandas Apply: 100%|██████████| 1988/1988 [00:00<00:00, 13050.21it/s]\n",
            "Pandas Apply: 100%|██████████| 1988/1988 [00:00<00:00, 6851.08it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Titulo_tokens_clean</th>\n",
              "      <th>Descripcion_tokens_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>[the, guardian, va, sanchez, europa, necesita,...</td>\n",
              "      <td>[diario, britanico, publico, pasado, jueves, e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>[revelan, gobierno, negocio, liberacion, mirel...</td>\n",
              "      <td>[revelan, gobierno, negocio, liberacion, mirel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>[ahora, nunca, joan, fuster, estatuto, valenci...</td>\n",
              "      <td>[valencianismo, convoca, castello, fiesta, gra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>[iglesias, alienta, yolanda, diaz, erc, eh, bi...</td>\n",
              "      <td>[politica, igual, negociar, empresarios, negoc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>[puigdemont, seria, ninguna, tragedia, repetic...</td>\n",
              "      <td>[entrevista, punt, avui, lider, jxcat, desdram...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>[pnv, consolida, mayoria, pse, salva, papeles,...</td>\n",
              "      <td>[nacionalistas, consiguen, alcaldias, bilbao, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>[exconsejero, nuria, marin, pide, indulto, cas...</td>\n",
              "      <td>[familiares, aluden, honestidad, integridad, p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>[fiscalia, pide, prision, incondicional, siete...</td>\n",
              "      <td>[suprime, delito, rebelion, imputo, inicialmen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>[jose, manuel, perez, tornero, creador, televi...</td>\n",
              "      <td>[futuro, presidente, rtve, licenciado, ciencia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>[ayusizacion, bng, santiago, abascal, instruye...</td>\n",
              "      <td>[pablo, santiago, abascal, planea, vivir, rent...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Label                                Titulo_tokens_clean  \\\n",
              "0      1  [the, guardian, va, sanchez, europa, necesita,...   \n",
              "1      0  [revelan, gobierno, negocio, liberacion, mirel...   \n",
              "2      1  [ahora, nunca, joan, fuster, estatuto, valenci...   \n",
              "3      1  [iglesias, alienta, yolanda, diaz, erc, eh, bi...   \n",
              "4      0  [puigdemont, seria, ninguna, tragedia, repetic...   \n",
              "5      1  [pnv, consolida, mayoria, pse, salva, papeles,...   \n",
              "6      0  [exconsejero, nuria, marin, pide, indulto, cas...   \n",
              "7      1  [fiscalia, pide, prision, incondicional, siete...   \n",
              "8      1  [jose, manuel, perez, tornero, creador, televi...   \n",
              "9      0  [ayusizacion, bng, santiago, abascal, instruye...   \n",
              "\n",
              "                            Descripcion_tokens_clean  \n",
              "0  [diario, britanico, publico, pasado, jueves, e...  \n",
              "1  [revelan, gobierno, negocio, liberacion, mirel...  \n",
              "2  [valencianismo, convoca, castello, fiesta, gra...  \n",
              "3  [politica, igual, negociar, empresarios, negoc...  \n",
              "4  [entrevista, punt, avui, lider, jxcat, desdram...  \n",
              "5  [nacionalistas, consiguen, alcaldias, bilbao, ...  \n",
              "6  [familiares, aluden, honestidad, integridad, p...  \n",
              "7  [suprime, delito, rebelion, imputo, inicialmen...  \n",
              "8  [futuro, presidente, rtve, licenciado, ciencia...  \n",
              "9  [pablo, santiago, abascal, planea, vivir, rent...  "
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def limpiar_y_procesar(data_set, columnas_texto):\n",
        "    \"\"\"\n",
        "    Función que aplica limpieza de datos y tokenización en columnas de texto específicas.\n",
        "\n",
        "    Parámetros:\n",
        "    - data_set (DataFrame): Dataset original con las columnas a procesar.\n",
        "    - columnas_texto (list): Lista de nombres de columnas que contienen texto a limpiar.\n",
        "\n",
        "    Retorna:\n",
        "    - DataFrame con los textos procesados.\n",
        "    \"\"\"\n",
        "    # Aplicar limpieza de datos\n",
        "    data_set_limpio = limpiar_data(data_set)\n",
        "    print(\" Data limpia\")\n",
        "\n",
        "    # Aplicar tokenización, lematización y normalización\n",
        "    df_procesado = procesar_texto(data_set_limpio, columnas_texto)\n",
        "    \n",
        "    return df_procesado\n",
        "\n",
        "# Uso de la función\n",
        "# df_procesado = limpiar_y_procesar(data_set, [\"Titulo\", \"Descripcion\"])\n",
        "df_procesado = limpiar_y_procesar(data_set_augmented, [\"Titulo\", \"Descripcion\"])\n",
        "df_procesado.head(10)\n",
        "\n",
        "# df = limpiar_y_procesar(data_set, [\"Titulo\", \"Descripcion\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.3.4 Vectorización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                 Titulo_tokens_clean  \\\n",
            "0  the guardian va sanchez europa necesita apuest...   \n",
            "1  revelan gobierno negocio liberacion mireles ca...   \n",
            "2  ahora nunca joan fuster estatuto valenciano cu...   \n",
            "3  iglesias alienta yolanda diaz erc eh bildu neg...   \n",
            "4  puigdemont seria ninguna tragedia repeticion e...   \n",
            "\n",
            "                            Descripcion_tokens_clean  \n",
            "0  diario britanico publico pasado jueves editori...  \n",
            "1  revelan gobierno negocio liberacion mireles ca...  \n",
            "2  valencianismo convoca castello fiesta grande c...  \n",
            "3  politica igual negociar empresarios negociar g...  \n",
            "4  entrevista punt avui lider jxcat desdramatizad...  \n"
          ]
        }
      ],
      "source": [
        "# Convertir listas de tokens a strings correctamente\n",
        "df_procesado[\"Titulo_tokens_clean\"] = df_procesado[\"Titulo_tokens_clean\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)\n",
        "df_procesado[\"Descripcion_tokens_clean\"] = df_procesado[\"Descripcion_tokens_clean\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)\n",
        "\n",
        "# Convertir listas de tokens a strings correctamente\n",
        "# df[\"Titulo_tokens_clean\"] = df[\"Titulo_tokens_clean\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)\n",
        "# df[\"Descripcion_tokens_clean\"] = df[\"Descripcion_tokens_clean\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)\n",
        "\n",
        "# Revisar si ya están en formato string\n",
        "print(df_procesado[[\"Titulo_tokens_clean\", \"Descripcion_tokens_clean\"]].head())\n",
        "\n",
        "# df_train = df_procesado\n",
        "# df_test_uniform = df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Clase pipeline limpieza y preprocesamiento**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class LimpiezaPreprocesamiento1(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        from nltk.corpus import stopwords\n",
        "        self.stop_words = set(stopwords.words(\"spanish\"))\n",
        "\n",
        "    def definir_variables(self, df):\n",
        "        features = ['Titulo', 'Descripcion', 'Label']\n",
        "        return df[features] if \"Label\" in df.columns else df[[\"Titulo\", \"Descripcion\"]]\n",
        "\n",
        "    def remove_duplicates(self, df):\n",
        "        return df.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "    def eliminar_duplicados_parciales(self, df):\n",
        "        if \"Label\" not in df.columns:\n",
        "            return df\n",
        "        conteo_labels = df.groupby(['Titulo', 'Descripcion'])['Label'].nunique()\n",
        "        duplicados_parciales = conteo_labels[conteo_labels > 1].index\n",
        "        return df[~df.set_index(['Titulo', 'Descripcion']).index.isin(duplicados_parciales)].reset_index(drop=True)\n",
        "\n",
        "    def limpiar_data(self, df):\n",
        "        df_variables = self.definir_variables(df)\n",
        "        df_duplicados = self.remove_duplicates(df_variables)\n",
        "        return self.eliminar_duplicados_parciales(df_duplicados)\n",
        "\n",
        "    def preprocessing(self, words):\n",
        "        import re, unicodedata\n",
        "        from num2words import num2words\n",
        "\n",
        "        words = [word.lower() for word in words]\n",
        "        words = [num2words(word, lang=\"es\") if word.isdigit() else word for word in words]\n",
        "        words = [re.sub(r'[^\\w\\s]', '', word) for word in words if word]\n",
        "        words = [unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore') for word in words]\n",
        "        words = [word for word in words if word not in self.stop_words]\n",
        "        return [word for word in words if word.strip() != \"\"]\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        import pandas as pd\n",
        "        import spacy\n",
        "\n",
        "        df = X.copy()\n",
        "        df = self.limpiar_data(df)\n",
        "\n",
        "        df[\"Titulo\"] = df[\"Titulo\"].astype(str).fillna(\"\")\n",
        "        df[\"Descripcion\"] = df[\"Descripcion\"].astype(str).fillna(\"\")\n",
        "\n",
        "        nlp = spacy.load(\"es_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
        "\n",
        "        resultados_titulo = [\n",
        "            [token.text for token in doc if not token.is_space]\n",
        "            for doc in nlp.pipe(df[\"Titulo\"], batch_size=100)\n",
        "        ]\n",
        "        resultados_descripcion = [\n",
        "            [token.text for token in doc if not token.is_space]\n",
        "            for doc in nlp.pipe(df[\"Descripcion\"], batch_size=100)\n",
        "        ]\n",
        "\n",
        "        df[\"Titulo_tokens_clean\"] = [self.preprocessing(t) for t in resultados_titulo]\n",
        "        df[\"Descripcion_tokens_clean\"] = [self.preprocessing(d) for d in resultados_descripcion]\n",
        "\n",
        "        df[\"Titulo_tokens_clean\"] = df[\"Titulo_tokens_clean\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)\n",
        "        df[\"Descripcion_tokens_clean\"] = df[\"Descripcion_tokens_clean\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)\n",
        "\n",
        "        return df[[\"Titulo_tokens_clean\", \"Descripcion_tokens_clean\"]]\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Dataset de entrenamiento: 988 registros\n",
            "✅ Dataset de prueba uniforme (17k registros): 1000 registros (8500 fake news y 8500 verídicas)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\majoa\\AppData\\Local\\Temp\\ipykernel_7416\\3208827131.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.sample(n=n_samples, random_state=42)))\n"
          ]
        }
      ],
      "source": [
        "df_procesado = data_set\n",
        "\n",
        "# Definir la cantidad de muestras por clase\n",
        "n_samples = 500 #LO MOVI\n",
        "\n",
        "# Extraer 8500 noticias falsas y 8500 noticias verídicas para pruebas finales\n",
        "df_test_uniform = (df_procesado.groupby(\"Label\", group_keys=False)\n",
        "                    .apply(lambda x: x.sample(n=n_samples, random_state=42)))\n",
        "\n",
        "# Crear el nuevo dataset de entrenamiento sin los registros seleccionados para pruebas\n",
        "df_train = df_procesado.drop(df_test_uniform.index)\n",
        "\n",
        "# Verificar tamaños de los datasets\n",
        "print(f\"✅ Dataset de entrenamiento: {len(df_train)} registros\")\n",
        "print(f\"✅ Dataset de prueba uniforme (17k registros): {len(df_test_uniform)} registros (8500 fake news y 8500 verídicas)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para la vectorización de textos se hace uso de la librería TF-IDF (Term Frequency-Inverse Document Frequency). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de registros en X: 1988\n",
            "Número de etiquetas en y: 1988\n",
            " X y y tienen el mismo número de registros. OK.\n"
          ]
        }
      ],
      "source": [
        "from scipy.sparse import hstack\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Vectorización con TF-IDF\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=3000)\n",
        "X_titulo = vectorizer.fit_transform(df_procesado[\"Titulo_tokens_clean\"])\n",
        "X_descripcion = vectorizer.fit_transform(df_procesado[\"Descripcion_tokens_clean\"])\n",
        "\n",
        "#  Concatenar ambas representaciones en una sola matriz\n",
        "X = hstack([X_titulo, X_descripcion])\n",
        "\n",
        "# Etiquetas\n",
        "y = df_procesado[\"Label\"]\n",
        "\n",
        "# Verificar dimensiones de la matriz X y etiquetas y\n",
        "print(f\"Número de registros en X: {X.shape[0]}\")\n",
        "print(f\"Número de etiquetas en y: {y.shape[0]}\")\n",
        "\n",
        "# Comprobar si hay inconsistencia\n",
        "if X.shape[0] == y.shape[0]:\n",
        "    print(\" X y y tienen el mismo número de registros. OK.\")\n",
        "else:\n",
        "    print(\" Error: X y y tienen un número diferente de registros.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usw9mrH3Qp8Y"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **3. Modelado y evaluación**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jal7SSSFRCvo"
      },
      "source": [
        "### 3.3. Estudiante 3 (Julian Mondragón): Modelo 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "B1PZZ_hxRIrn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Ejecutando Cross-Validation con Random Forest usando df_train...\n",
            "\n",
            " Resultados de Cross-Validation con Random Forest en df_train:\n",
            " Accuracy: 0.8300\n",
            " Precision: 0.8063\n",
            " Recall: 0.9881\n",
            " F1-Score: 0.8880\n",
            "\n",
            " Matriz de Confusión:\n",
            "[[154 160]\n",
            " [  8 666]]\n",
            "\n",
            " Reporte de Clasificación en Cross-Validation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.49      0.65       314\n",
            "           1       0.81      0.99      0.89       674\n",
            "\n",
            "    accuracy                           0.83       988\n",
            "   macro avg       0.88      0.74      0.77       988\n",
            "weighted avg       0.85      0.83      0.81       988\n",
            "\n",
            "\n",
            " Tiempo total de entrenamiento: 2.36 minutos\n",
            "\n",
            "Resultados en el **Conjunto de Prueba Final (df_test_uniform)**:\n",
            " Accuracy: 0.7730\n",
            " Precision: 0.6883\n",
            " Recall: 0.9980\n",
            " F1-Score: 0.8147\n",
            "\n",
            " Matriz de Confusión:\n",
            "[[274 226]\n",
            " [  1 499]]\n",
            "\n",
            " Reporte de Clasificación en df_test_uniform:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.55      0.71       500\n",
            "           1       0.69      1.00      0.81       500\n",
            "\n",
            "    accuracy                           0.77      1000\n",
            "   macro avg       0.84      0.77      0.76      1000\n",
            "weighted avg       0.84      0.77      0.76      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import time\n",
        "from limpieza_transformer import LimpiezaPreprocesamiento\n",
        "import joblib\n",
        "\n",
        "# Medir tiempo de ejecución\n",
        "start_time = time.time()\n",
        "\n",
        "# SEPARAR el preprocesamiento de la vectorización \n",
        "tfidf_transformer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('tfidf_titulo', TfidfVectorizer(ngram_range=(1,3), max_features=6000), \"Titulo_tokens_clean\"),\n",
        "        ('tfidf_descripcion', TfidfVectorizer(ngram_range=(1,3), max_features=6000), \"Descripcion_tokens_clean\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Crear pipeline donde el vectorizador **solo se ajusta dentro de cada fold**\n",
        "modelo_rf = Pipeline([\n",
        "    ('limpieza', LimpiezaPreprocesamiento()),\n",
        "    ('vectorizacion', tfidf_transformer),\n",
        "    ('clasificador', RandomForestClassifier(n_estimators=100, random_state=42))  \n",
        "])\n",
        "\n",
        "# Definir K-Folds estratificado (10 folds)\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Cross-validation donde `TfidfVectorizer` se ajusta dentro de cada fold\n",
        "print(\" Ejecutando Cross-Validation con Random Forest usando df_train...\")\n",
        "\n",
        "y_pred_cv = cross_val_predict(modelo_rf, df_train, df_train[\"Label\"], cv=kf, n_jobs=1, verbose=1)\n",
        "\n",
        "# Calcular métricas en cross-validation\n",
        "accuracy_cv = accuracy_score(df_train[\"Label\"], y_pred_cv)\n",
        "precision_cv = precision_score(df_train[\"Label\"], y_pred_cv, average='binary')\n",
        "recall_cv = recall_score(df_train[\"Label\"], y_pred_cv, average='binary')\n",
        "f1_cv = f1_score(df_train[\"Label\"], y_pred_cv, average='binary')\n",
        "conf_matrix_cv = confusion_matrix(df_train[\"Label\"], y_pred_cv)\n",
        "\n",
        "# Mostrar resultados de validación cruzada\n",
        "print(\"\\n Resultados de Cross-Validation con Random Forest en df_train:\")\n",
        "print(f\" Accuracy: {accuracy_cv:.4f}\")\n",
        "print(f\" Precision: {precision_cv:.4f}\")\n",
        "print(f\" Recall: {recall_cv:.4f}\")\n",
        "print(f\" F1-Score: {f1_cv:.4f}\")\n",
        "print(\"\\n Matriz de Confusión:\")\n",
        "print(conf_matrix_cv)\n",
        "\n",
        "# Reporte de clasificación\n",
        "print(\"\\n Reporte de Clasificación en Cross-Validation:\")\n",
        "print(classification_report(df_train[\"Label\"], y_pred_cv))\n",
        "\n",
        "# Medir tiempo total de entrenamiento\n",
        "end_time = time.time()\n",
        "print(f\"\\n Tiempo total de entrenamiento: {(end_time - start_time) / 60:.2f} minutos\")\n",
        "\n",
        "# -------------------------------------------\n",
        "# **ENTRENAMIENTO FINAL con df_train y evaluación en df_test_uniform**\n",
        "# -------------------------------------------\n",
        "\n",
        "# Ahora entrenamos el modelo en todo df_train (sin test leakage)\n",
        "modelo_rf.fit(df_train, df_train[\"Label\"])\n",
        "# Guardar el pipeline en un archivo\n",
        "joblib.dump(modelo_rf, \"modelo_random_forest.pkl\")\n",
        "\n",
        "# Transformamos df_test_uniform usando los valores aprendidos en df_train\n",
        "y_pred_test = modelo_rf.predict(df_test_uniform)\n",
        "\n",
        "# Calcular métricas en prueba final\n",
        "accuracy_test = accuracy_score(df_test_uniform[\"Label\"], y_pred_test)\n",
        "precision_test = precision_score(df_test_uniform[\"Label\"], y_pred_test, average='binary')\n",
        "recall_test = recall_score(df_test_uniform[\"Label\"], y_pred_test, average='binary')\n",
        "f1_test = f1_score(df_test_uniform[\"Label\"], y_pred_test, average='binary')\n",
        "conf_matrix_test = confusion_matrix(df_test_uniform[\"Label\"], y_pred_test)\n",
        "\n",
        "# Mostrar resultados en el conjunto de prueba final\n",
        "print(\"\\nResultados en el **Conjunto de Prueba Final (df_test_uniform)**:\")\n",
        "print(f\" Accuracy: {accuracy_test:.4f}\")\n",
        "print(f\" Precision: {precision_test:.4f}\")\n",
        "print(f\" Recall: {recall_test:.4f}\")\n",
        "print(f\" F1-Score: {f1_test:.4f}\")\n",
        "print(\"\\n Matriz de Confusión:\")\n",
        "print(conf_matrix_test)\n",
        "\n",
        "# Reporte de clasificación en prueba final\n",
        "print(\"\\n Reporte de Clasificación en df_test_uniform:\")\n",
        "print(classification_report(df_test_uniform[\"Label\"], y_pred_test))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "uniandes",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
