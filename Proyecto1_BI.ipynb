{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncwsXz1pOXLB"
      },
      "source": [
        "# **PROYECTO 1 BI - DETECCIÓN DE FAKE NEWS**\n",
        "---\n",
        "\n",
        "Integrantes equipo #11:\n",
        "*   Estudiante #1: Juan Pablo Barón - 202210502\n",
        "*   Estudiante #2: María José Amorocho - 202220179\n",
        "*   Estudiante #3: Julian Mondragón - 202221122\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install num2words\n",
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\majoa\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\majoa\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './Data/fake_news_spanish-1k.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspanish\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 19\u001b[0m data_set \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Data/fake_news_spanish-1k.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m data_set\u001b[38;5;241m.\u001b[39mhead()\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[1;32mc:\\Users\\majoa\\anaconda3\\envs\\uniandes\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Data/fake_news_spanish-1k.csv'"
          ]
        }
      ],
      "source": [
        "from pandas import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import re, string, unicodedata\n",
        "from num2words import num2words\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "\n",
        "\n",
        "# Punkt permite separar un texto en frases.\n",
        "nltk.download('punkt')\n",
        "#Descargar palabras vacías (stopwords)\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"spanish\"))\n",
        "\n",
        "\n",
        "data_set = pd.read_csv('./Data/fake_news_spanish-2k.csv', delimiter=\";\")\n",
        "data_set.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vnEF0APPGvF"
      },
      "source": [
        "## **1. Entendimiento del negocio y enfoque analítico**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR_wD96zPbrD"
      },
      "source": [
        "### 1.1. Generalidades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERM0K1K1O6Na"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXLMzYS7PiYA"
      },
      "source": [
        "### 1.2. Impacto y enfoque analítico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuRabCGzPk_L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs-0HUA1P8NJ"
      },
      "source": [
        "---\n",
        "\n",
        "## **2. Entendimiento y preparación de los datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Selección de variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para saber si una noticia es falsa o no, el modelo unicamente usará los datos de 'Titulo' y 'Descripcion' de la noticia. Pro una parte, la columna 'ID' dentro del dataset, no aporta ninguna información y por otro lado, la fecha de una noticia no es relevante para el caso dado que la verdacidad de la noticia no depende del día que haya sido escrita. Se dejará la columna de Label ya que a partir de esta es que el modelo puede aprender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1988, 3)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def definir_variables(data_set_inicial):\n",
        "    features = ['Titulo', 'Descripcion', 'Label']\n",
        "    return data_set_inicial[features]\n",
        "    \n",
        "df = definir_variables(data_set)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4reuwBZdQgBb"
      },
      "source": [
        "### 2.1. Perfilamiento de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Número de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uM7g2qJ3QCsj"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1988, 5)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_set.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tipos de dato presentes, cantidad de datos nulos y columnas que componen el data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1988 entries, 0 to 1987\n",
            "Data columns (total 5 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   ID           1988 non-null   object\n",
            " 1   Label        1988 non-null   int64 \n",
            " 2   Titulo       1988 non-null   object\n",
            " 3   Descripcion  1988 non-null   object\n",
            " 4   Fecha        1988 non-null   object\n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 77.8+ KB\n"
          ]
        }
      ],
      "source": [
        "data_set.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Datos vacíos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             Vacios\n",
            "ID                0\n",
            "Label             0\n",
            "Titulo            0\n",
            "Descripcion       0\n",
            "Fecha             0\n"
          ]
        }
      ],
      "source": [
        "df_null = pd.DataFrame(data_set.apply(lambda x: (x == \"\").sum()), columns=[\"Vacios\"])\n",
        "print(df_null)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Filas duplicadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0, 5)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "duplicated_rows = data_set.loc[data_set.duplicated(keep=False)]\n",
        "duplicated_rows.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Filas duplicadas parcialmente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0, 5)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def find_partial_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Identifica filas con el mismo título y descripción pero diferente label.\n",
        "    \n",
        "    Parámetros:\n",
        "    df (pd.DataFrame): DataFrame de entrada con columnas ['id', 'label', 'Titulo', 'descripcion', 'fecha'].\n",
        "    \n",
        "    Retorna:\n",
        "    pd.DataFrame: DataFrame con duplicados parciales.\n",
        "    \"\"\"\n",
        "    # Identificar duplicados basados en 'Titulo' y 'descripcion'\n",
        "    duplicated_mask = df.duplicated(subset=['Titulo', 'Descripcion'], keep=False)\n",
        "    \n",
        "    # Filtrar las filas que cumplen la condición\n",
        "    duplicates = df[duplicated_mask].sort_values(by=['Titulo', 'Descripcion'])\n",
        "    \n",
        "    # Agrupar por título y descripción, asegurando que hay más de un label diferente\n",
        "    duplicates_grouped = duplicates.groupby(['Titulo', 'Descripcion'])['Label'].nunique().reset_index()\n",
        "    \n",
        "    # Filtrar los casos donde hay más de un label único\n",
        "    filtered_duplicates = duplicates_grouped[duplicates_grouped['Label'] > 1]\n",
        "\n",
        "    # Unir con el DataFrame original para obtener los duplicados completos\n",
        "    result = df.merge(filtered_duplicates[['Titulo', 'Descripcion']], on=['Titulo', 'Descripcion'])\n",
        "\n",
        "    return result\n",
        "\n",
        "df = find_partial_duplicates(data_set)\n",
        "df.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HywvPf5FQi0x"
      },
      "source": [
        "### 2.2. Preparación de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Limpieza de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Eliminar filas duplicadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1988, 5)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def remove_duplicates(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Elimina las filas duplicadas del dataSet.\n",
        "    \n",
        "    Parámetros:\n",
        "    df (pd.DataFrame): DataFrame de entrada.\n",
        "    \n",
        "    Retorna:\n",
        "    pd.DataFrame: DataFrame sin filas duplicadas.\n",
        "    \"\"\"\n",
        "    return df.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "df = remove_duplicates(data_set)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Eliminar filas duplicadas parcialmente (noticias con un mismo titular y descripción, pero diferente label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1988, 5)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def eliminar_duplicados_parciales(df):\n",
        "    \"\"\"\n",
        "    Elimina filas donde 'Titulo' y 'Descripcion' sean iguales, pero 'Label' sea diferente.\n",
        "    \n",
        "    Parámetros:\n",
        "    df (pd.DataFrame): DataFrame con las columnas 'Titulo', 'Descripcion' y 'Label'.\n",
        "    \n",
        "    Retorna:\n",
        "    pd.DataFrame: DataFrame sin los duplicados parciales.\n",
        "    \"\"\"\n",
        "    # Contar cuántos valores únicos de Label existen por cada combinación de Titulo y Descripcion\n",
        "    conteo_labels = df.groupby(['Titulo', 'Descripcion'])['Label'].nunique()\n",
        "    \n",
        "    # Identificar las combinaciones que tienen más de un Label distinto (es decir, duplicados parciales)\n",
        "    duplicados_parciales = conteo_labels[conteo_labels > 1].index\n",
        "    \n",
        "    # Filtrar el DataFrame eliminando estas combinaciones\n",
        "    df_filtrado = df[~df.set_index(['Titulo', 'Descripcion']).index.isin(duplicados_parciales)]\n",
        "    \n",
        "    return df_filtrado.reset_index(drop=True)\n",
        "\n",
        "df = eliminar_duplicados_parciales(data_set)\n",
        "df.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A continuación, se muestra un método para hacer la limpieza completa de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1988, 3)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def limpiar_data(data_set):\n",
        "    df_variables = definir_variables(data_set)\n",
        "    df_duplicados = remove_duplicates(df_variables)\n",
        "    df_limpio = eliminar_duplicados_parciales(df_duplicados)\n",
        "    return df_limpio\n",
        "\n",
        "df = limpiar_data(data_set)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Eliminación de ruido"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Funciones para dejar la información en texto plano (remover caracteres no ASCII, convertir las palabras en minúscula, remover la puntuación, reemplazar los números y remover stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word is not None:\n",
        "            new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for w in words:\n",
        "        new_w = w.lower()\n",
        "        new_words.append(new_w)\n",
        "    return new_words\n",
        "\n",
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word is not None:\n",
        "            new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "            if new_word != '':\n",
        "                new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def replace_numbers(words):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word.isdigit():\n",
        "            new_word = num2words(word, lang=\"es\")\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stop_words:\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def preprocessing(words):\n",
        "    words = to_lowercase(words)\n",
        "    words = replace_numbers(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = remove_non_ascii(words)\n",
        "    words = remove_stopwords(words)\n",
        "    return words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Tokenización, lematización y normalización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para procesar los títulos de las noticias y su descripción se procede a ejecutar los siguietes pasos:\n",
        "1. **Tokenización**: Dividir frases u oraciones en palabras con el fin de desglozar las palabras correctamente para el posterior análisis\n",
        "\n",
        "2. **Lematización**: Cada palabra se reduce a su forma base o lema\n",
        "\n",
        "3. **Normalización**: Se hace uso de la función de reducción de ruido para colocar cada palaba en minuscula, sin tildes, remover los signos de puntuación, caracteres no ASCII, reemplazar los números y las palabras vacías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data limpia\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Titulo_tokens_clean</th>\n",
              "      <th>Descripcion_tokens_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>[the, guardian, va, sanchez, europa, necesita,...</td>\n",
              "      <td>[diario, britanico, publico, pasado, jueves, e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>[revelan, gobierno, negocio, liberacion, mirel...</td>\n",
              "      <td>[revelan, gobierno, negocio, liberacion, mirel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>[ahora, nunca, joan, fuster, estatuto, valenci...</td>\n",
              "      <td>[valencianismo, convoca, castello, fiesta, gra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>[iglesias, alienta, yolanda, diaz, erc, eh, bi...</td>\n",
              "      <td>[politica, igual, negociar, empresarios, negoc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>[puigdemont, seria, ninguna, tragedia, repetic...</td>\n",
              "      <td>[entrevista, punt, avui, lider, jxcat, desdram...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>[pnv, consolida, mayoria, pse, salva, papeles,...</td>\n",
              "      <td>[nacionalistas, consiguen, alcaldias, bilbao, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>[exconsejero, nuria, marin, pide, indulto, cas...</td>\n",
              "      <td>[familiares, aluden, honestidad, integridad, p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>[fiscalia, pide, prision, incondicional, siete...</td>\n",
              "      <td>[suprime, delito, rebelion, imputo, inicialmen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>[jose, manuel, perez, tornero, creador, televi...</td>\n",
              "      <td>[futuro, presidente, rtve, licenciado, ciencia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>[ayusizacion, bng, santiago, abascal, instruye...</td>\n",
              "      <td>[pablo, santiago, abascal, planea, vivir, rent...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Label                                Titulo_tokens_clean  \\\n",
              "0      1  [the, guardian, va, sanchez, europa, necesita,...   \n",
              "1      0  [revelan, gobierno, negocio, liberacion, mirel...   \n",
              "2      1  [ahora, nunca, joan, fuster, estatuto, valenci...   \n",
              "3      1  [iglesias, alienta, yolanda, diaz, erc, eh, bi...   \n",
              "4      0  [puigdemont, seria, ninguna, tragedia, repetic...   \n",
              "5      1  [pnv, consolida, mayoria, pse, salva, papeles,...   \n",
              "6      0  [exconsejero, nuria, marin, pide, indulto, cas...   \n",
              "7      1  [fiscalia, pide, prision, incondicional, siete...   \n",
              "8      1  [jose, manuel, perez, tornero, creador, televi...   \n",
              "9      0  [ayusizacion, bng, santiago, abascal, instruye...   \n",
              "\n",
              "                            Descripcion_tokens_clean  \n",
              "0  [diario, britanico, publico, pasado, jueves, e...  \n",
              "1  [revelan, gobierno, negocio, liberacion, mirel...  \n",
              "2  [valencianismo, convoca, castello, fiesta, gra...  \n",
              "3  [politica, igual, negociar, empresarios, negoc...  \n",
              "4  [entrevista, punt, avui, lider, jxcat, desdram...  \n",
              "5  [nacionalistas, consiguen, alcaldias, bilbao, ...  \n",
              "6  [familiares, aluden, honestidad, integridad, p...  \n",
              "7  [suprime, delito, rebelion, imputo, inicialmen...  \n",
              "8  [futuro, presidente, rtve, licenciado, ciencia...  \n",
              "9  [pablo, santiago, abascal, planea, vivir, rent...  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cargar modelo spaCy\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Función optimizada para tokenizar varias columnas\n",
        "def tokenizar_columnas(df, columnas):\n",
        "    \"\"\"\n",
        "    Tokeniza varias columnas en un DataFrame usando spaCy.\n",
        "    \n",
        "    Parámetros:\n",
        "    - df (pd.DataFrame): DataFrame con los datos.\n",
        "    - columnas (list): Lista de nombres de columnas a tokenizar.\n",
        "\n",
        "    Retorna:\n",
        "    - DataFrame con nuevas columnas de tokens.\n",
        "    \"\"\"\n",
        "    # Convertir las columnas a string y reemplazar NaN por \"\"\n",
        "    for col in columnas:\n",
        "        df[col] = df[col].astype(str).fillna(\"\")\n",
        "\n",
        "    # Aplicar `nlp.pipe()` en paralelo para mayor velocidad\n",
        "    for col in columnas:\n",
        "        df[f\"{col}_tokens\"] = [\n",
        "            [token.text for token in doc if not token.is_space] \n",
        "            for doc in nlp.pipe(df[col], batch_size=50)\n",
        "        ]\n",
        "    \n",
        "    return df\n",
        "\n",
        "def procesar_texto(df, columnas):\n",
        "    \"\"\"\n",
        "    Tokeniza, lematiza y aplica preprocesamiento a múltiples columnas de un DataFrame con spaCy.\n",
        "    \n",
        "    Parámetros:\n",
        "    - df (pd.DataFrame): DataFrame con los datos.\n",
        "    - columnas (list): Lista de nombres de columnas a procesar.\n",
        "\n",
        "    Retorna:\n",
        "    - Nuevo DataFrame con solo las columnas tokenizadas, lematizadas y procesadas, con 'label' como primera columna.\n",
        "    \"\"\"\n",
        "    # Convertir las columnas a string y reemplazar NaN por \"\"\n",
        "    for col in columnas:\n",
        "        df[col] = df[col].astype(str).fillna(\"\")\n",
        "\n",
        "    # Aplicar `nlp.pipe()` una sola vez por columna para eficiencia\n",
        "    datos_procesados = {}\n",
        "    \n",
        "    for col in columnas:\n",
        "        resultados = [\n",
        "            ([token.text for token in doc if not token.is_space],  # Tokens\n",
        "             [token.lemma_ for token in doc if not token.is_space])  # Lemas\n",
        "            for doc in nlp.pipe(df[col], batch_size=50)\n",
        "        ]\n",
        "\n",
        "        # Separar tokens y lemas en listas\n",
        "        df[f\"{col}_tokens\"], df[f\"{col}_lemmas\"] = zip(*resultados)\n",
        "\n",
        "        # Aplicar la función `preprocessing` sobre los tokens\n",
        "        datos_procesados[f\"{col}_tokens_clean\"] = df[f\"{col}_tokens\"].apply(preprocessing)\n",
        "\n",
        "    # Crear nuevo DataFrame solo con las columnas procesadas + label\n",
        "    nuevo_df = pd.DataFrame(datos_procesados)\n",
        "\n",
        "    # Agregar la columna label como la primera\n",
        "    nuevo_df.insert(0, \"Label\", df[\"Label\"])  \n",
        "\n",
        "    return nuevo_df\n",
        "\n",
        "\n",
        "# Aplicar tokenización\n",
        "data_setC = limpiar_data(data_set)\n",
        "print(\"data limpia\")\n",
        "df = procesar_texto(data_setC, [\"Titulo\", \"Descripcion\"])\n",
        "df.head(10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Vectorización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usw9mrH3Qp8Y"
      },
      "source": [
        "---\n",
        "\n",
        "## **3. Modelado y evaluación**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7FRRWnmQzCQ"
      },
      "source": [
        "### 3.1. Estudiante 1 (Juan Pablo Barón): Modelo 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFljfw-WQsV-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcdT6wSUQ7Rp"
      },
      "source": [
        "### 3.2. Estudiante 2 (María José Amorocho): Modelo 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjzrMKngQ_ke"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jal7SSSFRCvo"
      },
      "source": [
        "### 3.3. Estudiante 3 (Julian Mondragón): Modelo 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1PZZ_hxRIrn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L78_bpbWR5Uv"
      },
      "source": [
        "## **4. Resultados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pwOPX_OR82t"
      },
      "source": [
        "### 4.1. Descripción de resultados obtenidos y cómo aportan a lograr el objetivo del modelo planteado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NckxQoLiR7vv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usVY1cNiSI87"
      },
      "source": [
        "### 4.2. Estrategia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of9N_XUMSLI-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoG224E0SWsS"
      },
      "source": [
        "### 4.3 Archivo de predicciones sobre los datos de prueba en formato CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrTNtDtfSfrN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "uniandes",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
